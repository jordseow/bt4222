{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to 300 tweets were collected for each movies. This was done in order to gain further insight on the masses opinions on the movies. As Twitter is popular as a microblogging platform, it was the perfect place to garner opinions. We used GetOldTweets3, an API to fetch tweets through results from the search engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Time\n",
    "import time\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "#Importing Tweepy\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "#Import Pandas\n",
    "import pandas as pd\n",
    "#Import CSV\n",
    "import csv\n",
    "#Import module to retrieve tweets\n",
    "import GetOldTweets3 as got\n",
    "import sys\n",
    "username = sys.argv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Preprocessing\n",
    "#Importing pandas\n",
    "import pandas as pd\n",
    "#Import string for list of punctuation\n",
    "import string\n",
    "#Import natural language toolkit\n",
    "import nltk\n",
    "#import list of stop words\n",
    "from nltk.corpus import stopwords\n",
    "#import tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#import lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import numpy\n",
    "import numpy as np\n",
    "import spacy\n",
    "pd.options.mode.chained_assignment = None\n",
    "import emoji\n",
    "import emot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "We performed EDA on the dataset and here are the following observations: \n",
    "\n",
    "1) Since Twitter was only founded in mid 2006 and because we wanted to retrieve tweets within 6 months before and after the release, we had to drop movies that were before 2007.\n",
    "\n",
    "2) Replaced the '-' in the release date to '/' as the API reads in dates in the format 'YYYY/MM/DD'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data\n",
    "df = pd.read_csv(\"tmdb_movies_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing movies before 2007 and changing datetime format\n",
    "df = df[(df[\"release_year\"] >= 2007)\n",
    "df['release_date'] = pd.to_datetime(df['release_date'], utc = False)\n",
    "df['release_date'] = df['release_date'].astype(str)\n",
    "df['release_date'] = df['release_date'].str.replace(\"-\",\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving dates 6 months before and after movie \n",
    "for row,line in df.iterrows():\n",
    "        rows = []\n",
    "        movie_name = line['original_title']\n",
    "        rows.append(movie_name)\n",
    "        released = str(line['release_date'])\n",
    "        print(released)\n",
    "        datetime_object = datetime.strptime(released, '%Y/%m/%d')\n",
    "        date_from = (datetime_object + relativedelta(months=-6)).date()\n",
    "        date_until = (datetime_object + relativedelta(months=+6)).date()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Scraping Code\n",
    "\n",
    "We had to perform multiple for loops and put the model to sleep whenever HTTP Error occurs (too many requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter scraping code\n",
    "tweets_col = []\n",
    "searched_tweets = []\n",
    "last_id = -1\n",
    "\n",
    "def getTweets():\n",
    "    done = False\n",
    "    count = 0\n",
    "    end = len(df)\n",
    "    loop_count = 1\n",
    "    for i in range(0,100):\n",
    "            try:\n",
    "                print(\"starting loop: \",loop_count)\n",
    "                with open ('twitterdata_{}.csv'.format(loop_count), 'w', newline = \"\") as f:\n",
    "                    thewriter = csv.writer(f) \n",
    "                    thewriter.writerow(['Movie_Title', 'Date', \"Tweet_Date\",\"Tweet_Content\", \"Num_Retweets\", \"Hashtags\"])\n",
    "                    for row in range(count,end):\n",
    "                        if (count == end - 1):\n",
    "                            done = True\n",
    "                        rows = []\n",
    "                        movie_name = df.iloc[row]['original_title']\n",
    "                        rows.append(movie_name)\n",
    "                        released = str(df.iloc[row]['release_date'])\n",
    "                        rows.append(released)\n",
    "                        datetime_object = datetime.strptime(released, '%Y/%m/%d')\n",
    "                        date_from = str((datetime_object + relativedelta(months=-6)).date())\n",
    "                        date_until = str((datetime_object + relativedelta(months=+6)).date())\n",
    "                        tweetCriteria = got.manager.TweetCriteria().setQuerySearch(movie_name.lower()).setSince(date_from).setUntil(date_until).setMaxTweets(300).setLang('en')\n",
    "                        tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "                        print(\"movies done: \",count,)\n",
    "                        count += 1\n",
    "                        for i in range (len(tweet)):\n",
    "                            rows.append(tweet[i].date)\n",
    "                            rows.append(tweet[i].text)\n",
    "                            rows.append(tweet[i].retweets)\n",
    "                            rows.append(tweet[i].hashtags)\n",
    "                            thewriter.writerow(rows)\n",
    "                            rows = rows[:2]\n",
    "            except:\n",
    "                print(\"HHTPerror: sleeping for 300secs\")\n",
    "                loop_count +=1\n",
    "                time.sleep(300)\n",
    "                continue\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweet_content.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Results\n",
    "\n",
    "After exporting the results into a new csv, it was time to clean the csv in order to prepare for Sentiment Analysis. Here is what we did:\n",
    "\n",
    "1) Drop duplicate rows with same tweet content \n",
    "\n",
    "2) Convert tweets to lowercase\n",
    "\n",
    "3) Remove punctuation from tweets\n",
    "\n",
    "4) Remove digits from tweets\n",
    "\n",
    "5) Remove individual movie names from tweets\n",
    "\n",
    "6) Remove stop words from tweets\n",
    "\n",
    "7) Remove HTTP links from tweets\n",
    " \n",
    "8) Convert emoticons to words from tweets\n",
    "\n",
    "9) Remove emojis from tweets\n",
    "\n",
    "10) Chatword conversion from tweets\n",
    "\n",
    "11) Lemmatize tweets\n",
    "\n",
    "12) Drop duplicate rows with same cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweet_content.csv', engine = \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Dropping Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='Tweet_Content', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweet_Content\"] = df[\"Tweet_Content\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Converting Tweets to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned\"] = df[\"Tweet_Content\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Remove HTTP Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_http(text):\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned\"] = df[\"cleaned\"].apply(lambda x: remove_http(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned\"] = df[\"cleaned\"].apply(lambda text: remove_punctuation(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Remove Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_2\"] = df[\"cleaned\"].apply(lambda x: \"\".join([i for i in x if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Remove Movie Names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loopcount = 1\n",
    "for i in range(0, len(df)):\n",
    "    print (\"sentiment done:\", loopcount)\n",
    "    name = df['Movie_Title'].iloc[i].lower()\n",
    "    split = name.split()\n",
    "    big_regex = re.compile('|'.join(map(re.escape, split)))\n",
    "    the_message = big_regex.sub(\"\", df['cleaned_2'].iloc[i])\n",
    "    df['cleaned_2'].iloc[i] = the_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Removing Stopwords and 10 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words\n",
    "\", \".join(stopwords.words('english'))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "df[\"text_wo_stop\"] = df[\"cleaned_2\"].apply(lambda text: remove_stopwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing top 10 frequent words\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"text_wo_stop\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing top 10 frequent words\n",
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "df[\"cleaned_2\"] = df[\"text_wo_stop\"].apply(lambda text: remove_freqwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['text_wo_stop'])\n",
    "df['Cleaned_Tweets'] = df['cleaned_2']\n",
    "df = df.drop(columns = ['cleaned_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Converting Emoticons to Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emot\n",
    "EMOTICONS = emot.EMOTICONS\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()).replace(\"_\",\" \").lower(), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cleaned_Tweets\"] = df[\"Cleaned_Tweets\"].apply(lambda x: convert_emoticons(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cleaned_Tweets\"] = df[\"Cleaned_Tweets\"].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Chatword Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_str = \"\"\"\n",
    "AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatwords conversion\n",
    "\n",
    "chat_words_map_dict = {}\n",
    "\n",
    "chat_words_list = []\n",
    "\n",
    "for line in chat_words_str.split(\"\\n\"):\n",
    "    \n",
    "    if line != \"\":\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(chat_words_map_dict[w.upper()].lower())\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "chat_words_conversion(\"one minute BRB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cleaned_Tweets\"] = df[\"Cleaned_Tweets\"].apply(lambda x: chat_words_conversion(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Cleaned_Tweets'].notna()]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    \n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df[\"Cleaned_Tweets\"] = df[\"Cleaned_Tweets\"].apply(lambda text: lemmatize_words(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12) Dropping Duplicates from Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realised that there were a few duplicated tweets collected by the api and we remove them accordingly.\n",
    "df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])\n",
    "df = df.drop_duplicates(subset='Cleaned_Tweets', keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "We have decided to use Textblob to conduct sentiment analysis. After getting the sentiments for each tweet, we proceeded to average the polarity values out for each movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takess long to run\n",
    "def getSentiment():\n",
    "    loopcount = 1\n",
    "    for i in range(0, len(df)):\n",
    "        print (\"sentiment done:\", loopcount)\n",
    "        pol = TextBlob((df['Cleaned_Tweets'].iloc[i])).sentiment.polarity\n",
    "        subj = TextBlob((df['Cleaned_Tweets'].iloc[i])).sentiment.subjectivity\n",
    "        df.loc[df.index[i], 'polarity'] = pol\n",
    "        df.loc[df.index[i], 'subjectivity'] = subj\n",
    "        if pol > 0:\n",
    "            df.loc[df.index[i], 'sentiment'] = 'pos'\n",
    "        else:\n",
    "            df.loc[df.index[i], 'sentiment'] = 'neg'\n",
    "        loopcount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with the full tweet content and individual sentiment analysis\n",
    "df.to_csv('combined_pp.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging the polarity results \n",
    "df[\"polarity\"] = pd.to_numeric(df.polarity, errors='coerce')\n",
    "df_new = df.groupby(df['''tmdb_id''']).aggregate({'''polarity''':'mean'})\n",
    "df_new.drop(df_new.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with movies sentiment analysis\n",
    "df_new.to_csv('sentimental.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
